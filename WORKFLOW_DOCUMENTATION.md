# ğŸ“Š GeoPolitical Daily - KomplexnÃ­ Workflow Dokumentace

## ğŸ¯ PÅ™ehled SystÃ©mu
AutomatizovanÃ½ pipeline pro generovÃ¡nÃ­ dennÃ­ho geopolitickÃ©ho newsletteru z 26 zpravodajskÃ½ch zdrojÅ¯ s AI analÃ½zou a multi-platform publikovÃ¡nÃ­m.

---

## ğŸ”„ HLAVNÃ WORKFLOW PIPELINE

### ğŸ“ **SPOUÅ TÄšNÃ** (Trigger Points)

#### 1. **GitHub Actions Schedule** (`daily_newsletter.yml`)
- â° **DennÄ› v 6:00 UTC** (8:00 CET)
- AutomatickÃ½ trigger pÅ™es cron: `0 6 * * *`

#### 2. **ManuÃ¡lnÃ­ SpuÅ¡tÄ›nÃ­**
- GitHub Actions: Workflow dispatch s DRY_RUN volbou
- CLI: `python src/main_pipeline.py`
- Test: `python test_complete_pipeline.py`

#### 3. **Push Trigger**
- PÅ™i zmÄ›nÃ¡ch v `src/**`, `requirements.txt`, workflow souborech
- Ignoruje zmÄ›ny v `docs/**` (generovanÃ½ obsah)

---

## ğŸ—ï¸ PIPELINE ARCHITEKTURA

### ğŸ“ **HLAVNÃ ORCHESTRÃTOR** (`src/main_pipeline.py`)

```
run_complete_pipeline()
    â”œâ”€â”€ Step 0: Duplicate Check (line 64-80)
    â”œâ”€â”€ Step 1: Config Validation (line 83-129)
    â”œâ”€â”€ Step 2: Article Collection (line 131-202)
    â”œâ”€â”€ Step 2.5: Quality Validation (line 204-251)
    â”œâ”€â”€ Step 3: Processing & Clustering (line 253-312)
    â”œâ”€â”€ Step 4: AI Analysis (line 314-413)
    â”œâ”€â”€ Step 5: Newsletter Generation (line 415-468)
    â”œâ”€â”€ Step 6: Publishing (line 470-523)
    â”œâ”€â”€ Step 7: Email Notification (line 525-560)
    â”œâ”€â”€ Step 8: Summary Report (line 562-657)
    â””â”€â”€ Step 9: Cleanup (line 617-657)
```

---

## ğŸ“Š DETAILNÃ WORKFLOW KROKY

### **STEP 0: DUPLICATE CHECK** ğŸ”
**CÃ­l**: ZabrÃ¡nit duplicitnÃ­m newsletterÅ¯m
```python
# Kontroluje existenci: docs/newsletters/newsletter-{YYYY-MM-DD}.html
if newsletter_path.exists():
    logger.warning("Newsletter already exists")
    return True  # ÃšspÄ›Å¡nÃ½ exit bez zpracovÃ¡nÃ­
```

### **STEP 1: CONFIGURATION & RESILIENCE** âš™ï¸
**Komponenty**:
1. **Config Validation** (`Config.validate_config()`)
   - Kontrola API klÃ­ÄÅ¯
   - OvÄ›Å™enÃ­ sources.json
   - Environment variables

2. **Resilience Infrastructure**:
   ```python
   degradation_manager.register_component("collection_system")
   degradation_manager.register_component("ai_analyzer")
   degradation_manager.register_component("newsletter_generator")
   degradation_manager.register_component("publishing_system")
   health_monitor.start_monitoring()
   ```

### **STEP 2: DATA COLLECTION** ğŸ“°
**Orchestrace**: `MainCollector.collect_all_articles()`

#### ParalelnÃ­ SbÄ›r Dat:
```python
ThreadPoolExecutor(max_workers=10):
    â”œâ”€â”€ RSS Collector (20 zdrojÅ¯)
    â”‚   â”œâ”€â”€ BBC, CNN, Reuters...
    â”‚   â””â”€â”€ Failover groups by category
    â””â”€â”€ Web Scraper (6 zdrojÅ¯)
        â”œâ”€â”€ CSS selektory
        â””â”€â”€ SSL verify=False (line 121)
```

#### Health Monitoring:
- `source_health_monitor.register_source()`
- AutomatickÃ½ failover pÅ™i selhÃ¡nÃ­
- Circuit breakers pro kaÅ¾dÃ½ zdroj

**VÃ½stup**: ~400-500 raw articles

### **STEP 2.5: CONTENT QUALITY VALIDATION** âœ…
**Komponenta**: `content_quality_validator.validate_articles()`

```python
ValidaÄnÃ­ kritÃ©ria:
- MinimÃ¡lnÃ­ dÃ©lka titulku: 10 znakÅ¯
- MinimÃ¡lnÃ­ dÃ©lka summary: 50 znakÅ¯
- ValidnÃ­ URL
- Datum publikace (max 7 dnÃ­ starÃ©)
- DuplikÃ¡tnÃ­ obsah check
```

**VÃ½stup**: FiltrovÃ¡no na high-quality articles

### **STEP 3: PROCESSING & CLUSTERING** ğŸ”§
**Komponenta**: `MainProcessor.process_articles()`

#### Sub-kroky:
1. **Deduplication** (`ArticleDeduplicator`)
   - Title similarity (difflib.SequenceMatcher)
   - Threshold: 0.85 podobnost
   - VÃ½sledek: ~1.5% dedup rate

2. **Basic Scoring**:
   ```python
   high_priority_keywords = ['china', 'russia', 'ukraine', 'nato'...]
   source_weights = {'think_tank': 1.3, 'analysis': 1.1...}
   ```

3. **Clustering** (`cluster_articles()`)
   - SeskupenÃ­ podobnÃ½ch ÄlÃ¡nkÅ¯
   - Main article selection
   - Cluster scoring

**VÃ½stup**: ~50-100 clusters

### **STEP 4: AI ANALYSIS** ğŸ¤–
**Komponenta**: `ClaudeAnalyzer.analyze_clusters()`

#### Cost Control Flow:
```python
1. estimate_cost() â†’ Check budget
2. ai_cost_controller.check_budget_allowance()
3. If over budget â†’ Mock fallback
4. Else â†’ Claude API call
```

#### Claude API Integration:
- Model: `claude-3-haiku-20240307`
- Max tokens: 8000 (production)
- Temperature: 0.7
- Cost tracking per call

#### Fallback Mechanism:
```python
if Config.DRY_RUN or !API_KEY or budget_exceeded:
    return create_mock_analyses()
```

**VÃ½stup**: 4 top stories s AI analÃ½zou

### **STEP 5: NEWSLETTER GENERATION** ğŸ“§
**Komponenta**: `NewsletterGenerator.generate_newsletter()`

#### Content Balancing:
```python
target_breaking = 25%  # Breaking news
remaining = 75%        # Analysis + Trends
```

#### HTML Generation:
- Template-based rendering
- Impact score color coding
- Professional styling
- Mobile responsive

**VÃ½stup**: HTML newsletter + metadata

### **STEP 6: MULTI-PLATFORM PUBLISHING** ğŸš€
**Komponenta**: `GitHubPagesPublisher.publish_newsletter()`

#### Publishing Targets:
1. **GitHub Pages** (`docs/`)
   - Newsletter: `docs/newsletters/newsletter-{date}.html`
   - Index: `docs/index.html`
   - Dashboard: `docs/dashboard.html`
   - Sitemap: `docs/sitemap.xml`

2. **Legacy File** (`output/`)
   - Backup: `output/newsletter_{timestamp}.html`

### **STEP 7: EMAIL NOTIFICATION** ğŸ“¬
**Komponenta**: `EmailNotifier.notify_newsletter_ready()`

```python
if Config.SMTP_SERVER and Config.ADMIN_EMAIL:
    send_email_notification()
else:
    logger.info("Email not configured")
```

### **STEP 8: METRICS & REPORTING** ğŸ“ˆ
**Komponenty**:
- `MetricsCollector`: SQLite databÃ¡ze
- `ai_cost_controller.get_cost_report()`
- `pipeline_tracker.track_pipeline_success()`

#### Metriky:
- Articles collected/processed
- Deduplication rate
- AI costs (daily/monthly)
- Processing time
- Error rates

### **STEP 9: CLEANUP** ğŸ§¹
**Komponenta**: `CleanupManager.run_full_cleanup()`

- Retention: 30 dnÃ­
- ÄŒistÃ­: logs, output, old newsletters
- Database pruning
- Pouze v production mode

---

## ğŸ›¡ï¸ ERROR HANDLING & RESILIENCE

### **Circuit Breakers**
```python
CircuitBreaker(
    failure_threshold=5,
    recovery_timeout=60.0,
    success_threshold=3
)
```

### **Retry Logic**
```python
@retry_on_error(
    max_retries=3,
    backoff_factor=2.0,
    jitter=True
)
```

### **Graceful Degradation**
```python
if degradation_manager.should_skip_operation():
    use_fallback_or_skip()
```

### **Health Monitoring**
- Continuous health checks
- Auto-recovery attempts
- Alert generation on failures

---

## ğŸ“Š PERFORMANCE CHARAKTERISTIKY

### **TypickÃ© ÄŒasy BÄ›hu**:
- Collection: ~7-10 sekund
- Processing: ~25-30 sekund
- AI Analysis: ~5-15 sekund
- Publishing: ~2-5 sekund
- **CELKEM: ~45-60 sekund**

### **Resource Usage**:
- Memory: ~200-300 MB
- CPU: 1-2 cores bÄ›hem collection
- Network: ~10-20 MB download
- AI tokens: ~8000-15000 per run

### **Success Rates**:
- Pipeline success: ~98.5%
- Source availability: ~95%
- AI fallback rate: ~5%

---

## ğŸ” SECURITY & COMPLIANCE

### **API Key Management**:
- GitHub Secrets pro production
- .env file pro development
- Å½Ã¡dnÃ© klÃ­Äe v kÃ³du

### **SSL/TLS**:
âš ï¸ **KNOWN ISSUE**: Web scraper mÃ¡ `CERT_NONE` (line 121)
- DÅ¯vod: NÄ›kterÃ© zdroje majÃ­ expired certs
- Risk: MITM attacks (nÃ­zkÃ© pro trusted sources)
- TODO: Implementovat per-source SSL config

### **Data Privacy**:
- Å½Ã¡dnÃ© osobnÃ­ Ãºdaje
- Pouze veÅ™ejnÃ© zpravodajskÃ© ÄlÃ¡nky
- 30dennÃ­ retention policy

---

## ğŸš¨ MONITORING & ALERTING

### **GitHub Actions**:
- Issue creation on failure
- Artifact retention: 30 dnÃ­
- Email notifications (pokud nakonfigurovÃ¡no)

### **Logging**:
```python
Structured logging s:
- Correlation IDs
- Pipeline stages
- Error categories
- Performance metrics
```

### **Dashboard** (`docs/dashboard.html`):
- Collection statistics
- Processing metrics
- AI cost tracking
- Error trends

---

## ğŸ”§ TROUBLESHOOTING GUIDE

### **ÄŒastÃ© ProblÃ©my**:

1. **"Insufficient articles collected"**
   - Check source health
   - Verify network connectivity
   - Review RSS feed availability

2. **"AI budget exceeded"**
   - Check daily/monthly limits
   - Review cost_controller logs
   - Consider mock mode

3. **"Newsletter already exists"**
   - Normal behavior (duplicate prevention)
   - Check if manual cleanup needed

4. **SSL Certificate Errors**
   - Known issue with some sources
   - Currently bypassed with CERT_NONE
   - Monitor for security updates

---

## ğŸ“š DEPENDENCIES & INTEGRATIONS

### **External Services**:
- Anthropic Claude API
- GitHub Pages hosting
- SMTP server (optional)
- 26 news sources (RSS/Web)

### **Key Libraries**:
- `anthropic`: Claude AI client
- `beautifulsoup4`: Web scraping
- `feedparser`: RSS parsing
- `requests`: HTTP client
- `sqlite3`: Metrics storage

---

## ğŸ¯ CONFIGURATION MATRIX

| Variable | Production | Development | Test |
|----------|------------|-------------|------|
| DRY_RUN | false | true | true |
| AI_MAX_TOKENS | 8000 | 4000 | 1000 |
| CLEANUP_ENABLED | true | false | false |
| LOG_LEVEL | INFO | DEBUG | DEBUG |
| MAX_RETRIES | 3 | 2 | 1 |

---

## ğŸ“ˆ FUTURE IMPROVEMENTS

### **PlÃ¡novanÃ©**:
- [ ] Substack integration
- [ ] Per-source SSL configuration
- [ ] Real-time monitoring dashboard
- [ ] A/B testing for AI prompts
- [ ] User personalization

### **V Ãšvaze**:
- [ ] Multiple AI providers (Gemini backup)
- [ ] Semantic deduplication
- [ ] Sentiment analysis
- [ ] Multi-language support
- [ ] API endpoint pro external consumers

---

**PoslednÃ­ aktualizace**: 2025-09-03
**Verze dokumentace**: 1.0.0